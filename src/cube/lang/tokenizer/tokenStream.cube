use cube.lang.keyword
use cube.lang.cst
use cube.lang.parser.syntaxException
use java.util.arrayList
use java.util.list

define tokenStream
  function create(tokenizer as tokenizer) = me.tokenizer = tokenizer

  function next as token
    fill(0)
    output buffer.remove(0)
  end

  function expect(s as symbol) as token
    var t as token = next
    if t.isSymbol(s) then output t
    error new syntaxException("Expected '" + s.text + "' not " + t.nodeType + '.')
  end

  function expect(k as keyword) as token
    var t as token = next
    if t.isKeyword(k) then output t
    error new syntaxException("Expected '" + k.text + "' not " + t.nodeType + '.')
  end

  function read(s as symbol)
    if not peek(0).isSymbol(s) then output false
    next
    output true
  end

  function peek(n as int) as token
    fill(n)
    output buffer.get(n)
  end

  shared
    function tokenStream(text as string) = new tokenStream(new tokenizer(text))
  end

  internal
    let buffer = new list[token]
    var tokenizer as tokenizer

    function fill(n as int) =
      while buffer.size <= n
        var p as token = null

        if tokenizer.next then do
          var text as string = tokenizer.tokenText
          var t as cstNodeType = tokenizer.tokenType

          match t
            when SYMBOL then p = new symbolToken(tokenizer.symbol, text)
            when KEYWORD then p = new keywordToken(tokenizer.keyword, text)
            else p = new token(t, text)
          end
        end

        buffer.add(p)
      end
  end
end